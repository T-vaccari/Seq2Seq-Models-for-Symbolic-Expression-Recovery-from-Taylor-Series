{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simbolic Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import symbols, sin, cos, exp, series, simplify, expand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "As stated in the paper FASEROH : The key to FASEROH is generating a sufficiently diverse set of functions that span the space of functions\n",
    "used in particle physics and cosmology. Therefore, the first step is to create an inventory of the functions\n",
    "most commonly used in these fields, noting their typical parameterizations. We call this set the set of base\n",
    "functions, B. A tentative algorithm for generating functions is given in Algorithm 2.\n",
    "\n",
    "First of all to manage simbolic expression I am going to use sympy. The next step is to expand the randomic function created using algorithm 2 using taylor's expansion up to the fourth order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbol for Sympy expansions\n",
    "x = symbols('x', real=True)\n",
    "\n",
    "BASE_FUNCTIONS = [x, x**2, x**3, sin(x), cos(x), exp(x)]\n",
    "OPERATORS = ['+', '-', '*', '/']\n",
    "\n",
    "def generate_random_function():\n",
    "    \"\"\"\n",
    "    Generates a random function by combining base functions explicitly.\n",
    "    \"\"\"\n",
    "    num_terms = random.randint(1, 3)  \n",
    "    f = random.choice(BASE_FUNCTIONS)\n",
    "    \n",
    "    for _ in range(num_terms - 1):\n",
    "        operator = random.choice(OPERATORS)\n",
    "        next_function = random.choice(BASE_FUNCTIONS)\n",
    "        \n",
    "        if operator == '/':\n",
    "            # Ensure we are not dividing by an expression that can become zero\n",
    "            if next_function != 0 and next_function != x - x:  \n",
    "                f = f / next_function\n",
    "        elif operator == '+':\n",
    "            f = f + next_function\n",
    "        elif operator == '-':\n",
    "            f = f - next_function\n",
    "        elif operator == '*':\n",
    "            f = f * next_function\n",
    "\n",
    "    f = simplify(expand(f))\n",
    "    \n",
    "    # Avoid returning 0\n",
    "    if f == 0:\n",
    "        return generate_random_function()  # Ricrea una funzione se otteniamo zero\n",
    "    \n",
    "    return f\n",
    "\n",
    "def compute_taylor_expansion(f, x, order=4):\n",
    "    \"\"\"\n",
    "    Computes and simplifies the Taylor's series expansion of a function around x=0.\n",
    "    Default up to order=4.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expansion = series(f, x, 0, order+1).removeO()\n",
    "        return simplify(expansion)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def tokenize(expression_str):\n",
    "    \"\"\"\n",
    "    Tokenizes a mathematical expression into a list of tokens\n",
    "    \"\"\"\n",
    "    expression_str = str(expression_str).replace(\"**\", \"^\")  \n",
    "    \n",
    "    expression_str = re.sub(r'([+\\-*/^()])', r' \\1 ', expression_str)\n",
    "    \n",
    "    expression_str = re.sub(r'(\\s)-(\\d+(\\.\\d*)?)', r' \\1\\2', expression_str)\n",
    "\n",
    "    expression_str = re.sub(r'\\b(sin|cos|exp|log|tan|sqrt)\\s*\\(', r' \\1 ( ', expression_str)\n",
    "    \n",
    "    tokens = expression_str.strip().split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_samples=10000, filename=\"dataset.json\", taylor_order=6):\n",
    "    \"\"\"\n",
    "    Generates (function, taylor_expansion) pairs, each tokenized.\n",
    "    Saves them into a JSON file: [ { \"function\": [...], \"expansion\": [...] }, ... ]\n",
    "    This is usefull because we are going to use it later to train the LSTM\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        f = generate_random_function()\n",
    "        expansion = compute_taylor_expansion(f, x, order=taylor_order)\n",
    "        \n",
    "        f_str = str(f)\n",
    "        expansion_str = str(expansion)\n",
    "        \n",
    "        data.append({\n",
    "            \"function\": tokenize(f_str),\n",
    "            \"expansion\": tokenize(expansion_str)\n",
    "        })\n",
    "    \n",
    "    with open(filename, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "    \n",
    "\n",
    "\n",
    "# generate_dataset(num_samples=200000, filename=\"dataset.json\", taylor_order=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how our helper functions builds the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2*(x - 1)\n",
      "x*exp(x)\n",
      "x\n",
      "x**2 + sin(2*x)/2\n",
      "x*(x**2 + cos(x))\n",
      "(x**2 + cos(x))*exp(-x)\n",
      "exp(x)\n",
      "1\n",
      "exp(x)\n",
      "x**2/cos(x)\n"
     ]
    }
   ],
   "source": [
    "# Randomly creating a function\n",
    "for i in range(10):\n",
    "   print(generate_random_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x*(x**3 + x**2 + 6*x + 6)/6\n",
      "x**3 - x\n",
      "x**3\n",
      "x**4/24 + x**3/6 + x**2/2 + x + 1\n",
      "x**4/12 + x**3/6 + x + 2\n",
      "x*(-x**2/6 + x + 1)\n",
      "-x**3 + x\n",
      "x\n",
      "-x**4/6 + x**2\n"
     ]
    }
   ],
   "source": [
    "# Creating the taylor expansion\n",
    "for i in range(10):\n",
    "   print(compute_taylor_expansion(generate_random_function(),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', '^', '3', '/', '6']\n",
      "['x', '*', '(', 'x', '-', '1', ')']\n",
      "['7', '*', 'x', '^', '4', '/', '360', '+', 'x', '^', '2', '/', '6', '+', '1']\n",
      "['x', '^', '4', '/', '24', '+', 'x', '^', '3', '/', '6', '+', 'x', '^', '2', '/', '2', '+', 'x', '+', '1']\n",
      "['-', 'x', '^', '4', '/', '24', '-', 'x', '^', '3', '/', '3', '-', 'x', '^', '2', '/', '2', '-', '1']\n",
      "['x', '^', '4', '/', '120', '+', 'x', '^', '3', '/', '24', '+', 'x', '^', '2', '/', '6', '+', 'x', '/', '2', '+', '1', '+', '1', '/', 'x']\n",
      "['-', 'x', '^', '4', '/', '24', '+', '5', '*', 'x', '^', '3', '/', '6', '-', 'x', '^', '2', '/', '2', '-', 'x', '-', '1']\n",
      "['x', '^', '3']\n",
      "['x', '^', '4', '/', '24', '+', 'x', '^', '3', '/', '6', '+', 'x', '^', '2', '/', '2', '+', 'x', '+', '1']\n",
      "['x']\n"
     ]
    }
   ],
   "source": [
    "# How the dataset is tokenized\n",
    "for i in range(10):\n",
    "   print(tokenize(str(compute_taylor_expansion(generate_random_function(),x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a vocab, with unique entries to tokenize the dataset, knowing that we are going to need in a index format for training the LSTM(added also the token for SOS , EOS and PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 161\n",
      "Sample vocab items: [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3), ('(', 4), (')', 5), ('*', 6), ('+', 7), ('-', 8), ('/', 9), ('1', 10), ('10', 11), ('100', 12), ('10080', 13), ('103', 14), ('105', 15), ('1080', 16), ('11', 17), ('112', 18), ('114', 19)]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(\"dataset.json\", \"r\") as fp:\n",
    "    dataset = json.load(fp)\n",
    "\n",
    "\n",
    "all_tokens = set()\n",
    "for entry in dataset:\n",
    "    for t in entry[\"function\"]:\n",
    "        all_tokens.add(t)\n",
    "    for t in entry[\"expansion\"]:\n",
    "        all_tokens.add(t)\n",
    "\n",
    "\n",
    "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "for i, st in enumerate(special_tokens):\n",
    "    vocab[st] = i\n",
    "\n",
    "offset = len(special_tokens)  \n",
    "for idx, token in enumerate(sorted(all_tokens)):\n",
    "    \n",
    "    if token not in vocab:\n",
    "        vocab[token] = idx + offset\n",
    "\n",
    "# Build the reverse mapping\n",
    "idx_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "PAD_TOKEN = vocab[\"<PAD>\"]\n",
    "SOS_TOKEN = vocab[\"<SOS>\"]\n",
    "EOS_TOKEN = vocab[\"<EOS>\"]\n",
    "UNK_TOKEN = vocab[\"<UNK>\"]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Sample vocab items:\", list(vocab.items())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved numerical dataset with 137732 samples.\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_ids(tokens, vocab, unk_token=UNK_TOKEN):\n",
    "    return [vocab[token] if token in vocab else unk_token for token in tokens]\n",
    "\n",
    "numerical_data = []\n",
    "for entry in dataset:\n",
    "    func_ids = tokens_to_ids(entry[\"function\"], vocab)\n",
    "    exp_ids  = tokens_to_ids(entry[\"expansion\"], vocab)\n",
    "    numerical_data.append({\n",
    "        \"function\": func_ids,\n",
    "        \"expansion\": exp_ids\n",
    "    })\n",
    "\n",
    "with open(\"numerical_dataset.json\", \"w\") as fp:\n",
    "    json.dump(numerical_data, fp, indent=4)\n",
    "\n",
    "print(f\"Saved numerical dataset with {len(numerical_data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically now we have a dataset containing pair of fucntion and their taylor expansion. We already created the numerical dataset used to train the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    " \n",
    "Now the goal is to use the dataset created to train a type of RNN called LSTM. Briefly recall of what this architecture are : A Recurrent Neural Network (RNN) is a type of artificial neural network designed to handle sequential data, such as time series, text, speech, and videos. Unlike feedforward neural networks, which process inputs independently, RNNs have a built-in memory mechanism that allows them to maintain contextual information across different time steps.\n",
    "\n",
    "Key Characteristics of RNNs\n",
    "\n",
    "Sequential Processing – Unlike traditional neural networks that treat each input independently, RNNs process sequences step by step, maintaining a state (hidden representation) that captures information from previous time steps.\n",
    "Shared Weights – The same weights (parameters) are used at each time step, reducing the number of parameters compared to deep feedforward networks and making RNNs more efficient for sequential tasks.\n",
    "Hidden State – RNNs maintain a hidden state \n",
    "h\n",
    "t\n",
    "h \n",
    "t\n",
    "​\t\n",
    "  that acts as a memory, capturing past information.\n",
    "Backpropagation Through Time (BPTT) – The training of RNNs involves unrolling the network through time and applying backpropagation to update weights.\n",
    "Challenges of Standard RNNs\n",
    "\n",
    "\n",
    "Key Components of an LSTM Cell\n",
    "\n",
    "Forget Gate: Decides what portion of past information should be discarded.\n",
    "Input Gate: Determines how much new information should be stored in the memory.\n",
    "Cell State: Acts as a long-term memory that carries relevant information through time.\n",
    "Output Gate: Controls what information from the cell state should be output at the current step.\n",
    "By controlling the flow of information explicitly, LSTMs mitigate the vanishing gradient problem and improve the handling of long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline that we are going to apply for training the LSTM  : \n",
    "1. Creating our dataloader taylored for our purpose\n",
    "2. Defining the model with the parameters\n",
    "3. Training Loop\n",
    "4. Evaluation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaylorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the numerical data in memory. Each item is:\n",
    "    {\n",
    "      \"function\": [int, int, int, ...],\n",
    "      \"expansion\": [int, int, int, ...]\n",
    "    }\n",
    "    We ll make the expansion the encoder input, and function the decoder target.\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path):\n",
    "        super().__init__()\n",
    "        with open(json_path, \"r\") as fp:\n",
    "            self.data = json.load(fp)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        exp_seq = item[\"expansion\"]\n",
    "        func_seq = item[\"function\"]\n",
    "        \n",
    "        func_seq_in  = [SOS_TOKEN] + func_seq\n",
    "        func_seq_out = func_seq + [EOS_TOKEN]\n",
    "        \n",
    "        return {\n",
    "            \"expansion\": torch.tensor(exp_seq, dtype=torch.long),\n",
    "            \"function_in\": torch.tensor(func_seq_in, dtype=torch.long),\n",
    "            \"function_out\": torch.tensor(func_seq_out, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    We need to pad the sequence given the architecture.\n",
    "    Given a list of items from TaylorDataset.getitem(), produce:\n",
    "      exps_padded, exp_lengths, funcs_in_padded, funcs_out_padded, func_lengths\n",
    "    \"\"\"\n",
    "    exps      = [b[\"expansion\"]    for b in batch]\n",
    "    funcs_in  = [b[\"function_in\"]  for b in batch]\n",
    "    funcs_out = [b[\"function_out\"] for b in batch]\n",
    "    \n",
    "    exp_lengths  = torch.tensor([len(seq) for seq in exps],      dtype=torch.long)\n",
    "    func_lengths = torch.tensor([len(seq) for seq in funcs_in],  dtype=torch.long)\n",
    "    \n",
    "    exps_padded      = pad_sequence(exps,      batch_first=True, padding_value=PAD_TOKEN)\n",
    "    funcs_in_padded  = pad_sequence(funcs_in,  batch_first=True, padding_value=PAD_TOKEN)\n",
    "    funcs_out_padded = pad_sequence(funcs_out, batch_first=True, padding_value=PAD_TOKEN)\n",
    "    \n",
    "    return (\n",
    "        exps_padded,      # [B, max_len_exp]\n",
    "        exp_lengths,      # [B]\n",
    "        funcs_in_padded,  # [B, max_len_func]\n",
    "        funcs_out_padded, # [B, max_len_func]\n",
    "        func_lengths      # [B]\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_obj = TaylorDataset(\"numerical_dataset.json\")\n",
    "train_loader = DataLoader(\n",
    "    dataset_obj,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Batch ---\n",
      "Expansions (padded): \n",
      "tensor([[160, 153,  92,  ...,   0,   0,   0],\n",
      "        [160,   6,   4,  ...,   0,   0,   0],\n",
      "        [  8, 160, 153,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  4, 160, 153,  ...,   0,   0,   0],\n",
      "        [160, 153,  92,  ...,   0,   0,   0],\n",
      "        [160, 153,  92,  ...,   0,   0,   0]])\n",
      "Expansions lengths: tensor([13, 14,  8, 13,  9, 21,  8, 13, 21,  9,  8, 15, 14, 19,  8, 21, 38, 21,\n",
      "         8, 15, 21, 21, 21, 10, 15, 21, 13,  9, 13, 21, 13, 10, 11, 21, 27, 13,\n",
      "        23, 13, 20, 23, 13, 13, 24,  3, 25, 25,  9, 21, 16, 19, 19, 49, 17, 10,\n",
      "        16, 21, 13, 10, 17, 12,  9, 23, 10, 15, 11, 21, 16, 23, 13, 23, 13, 11,\n",
      "        22, 14, 10, 14, 25,  3, 21,  3, 45, 15, 21, 15,  9, 15, 20, 13, 14,  8,\n",
      "        13, 13, 15, 15,  9, 15, 21,  9,  9, 14, 24, 13, 27,  8, 16, 13, 10,  9,\n",
      "         5,  8, 21,  3, 10, 21, 27,  3,  8,  3, 15, 25, 22,  8, 15, 31, 14, 14,\n",
      "         8,  9,  5, 15, 13, 37, 21,  8,  8, 21, 13,  7, 11, 15,  8, 10, 13, 17,\n",
      "        14, 21, 17, 16, 16,  7, 10,  8, 19, 24, 14, 15, 29, 20,  7, 23, 37, 13,\n",
      "        13,  9,  8, 21,  8, 13, 15, 21, 21, 21, 13, 24, 16, 17, 21,  8, 15, 10,\n",
      "        13, 23, 25, 13, 19, 20,  3, 21,  9, 27, 17,  6,  5, 21, 10, 21,  8,  8,\n",
      "        13,  3,  8,  9, 20,  9, 13,  9, 19, 22, 13, 13,  8, 15, 21, 23,  3, 21,\n",
      "         8, 13, 16,  9, 23, 21, 21, 14,  9, 19, 21, 18,  9,  3,  8, 18,  9,  7,\n",
      "        21, 15, 18,  3, 21, 15, 13, 27, 15, 27, 15, 13, 18, 21, 13, 21,  8, 23,\n",
      "        15, 45, 15, 17])\n",
      "Function Inputs (padded): \n",
      "tensor([[  1, 154,   4,  ...,   0,   0,   0],\n",
      "        [  1, 160,   6,  ...,   0,   0,   0],\n",
      "        [  1, 157,   4,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  1,   4, 160,  ...,   0,   0,   0],\n",
      "        [  1,   8, 160,  ...,   0,   0,   0],\n",
      "        [  1, 155,   4,  ...,   0,   0,   0]])\n",
      "Function Outputs (padded): \n",
      "tensor([[154,   4, 160,  ...,   0,   0,   0],\n",
      "        [160,   6,   4,  ...,   0,   0,   0],\n",
      "        [157,   4, 160,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  4, 160, 153,  ...,   0,   0,   0],\n",
      "        [  8, 160,   7,  ...,   0,   0,   0],\n",
      "        [155,   4, 160,  ...,   0,   0,   0]])\n",
      "Function lengths: tensor([ 5, 13,  5, 14,  9,  5,  5,  9,  5,  9,  7, 12, 14,  8,  5,  5, 10,  5,\n",
      "         5,  7,  5,  5,  5,  7, 14, 12,  5, 15, 13,  5,  5,  9, 11, 11,  7,  5,\n",
      "         9,  5, 15,  9,  5,  5,  9,  9, 13, 10,  9, 22, 13, 13, 14, 15, 10,  7,\n",
      "        16,  5,  5, 10,  9,  7,  9, 14, 10,  9, 11,  5,  9,  7,  5,  9,  5,  7,\n",
      "         9, 10,  9,  9, 12,  9,  5,  9,  9,  9,  5,  7, 15, 11, 15,  5,  7,  5,\n",
      "         5,  7,  7,  9, 15,  7,  5, 15,  9, 10,  9,  5,  7,  5, 13,  5,  9,  9,\n",
      "        11,  5,  5,  9,  9,  5, 10, 14,  5,  9,  7, 12, 10,  5, 14, 14, 11,  9,\n",
      "         5,  7,  7, 12, 14, 12,  5,  5,  5,  5, 12,  6,  7,  7,  5,  9,  9, 14,\n",
      "         7,  5, 12, 10, 14,  7,  7,  5, 10,  9,  9, 13, 16, 12,  7,  9, 10,  5,\n",
      "         5, 15,  5,  5,  5,  5, 10,  5,  5, 11,  5,  9, 12, 10,  5,  5, 14,  7,\n",
      "         5,  7, 14,  5,  7,  7,  9, 11,  9,  7,  9, 15, 11,  5, 10,  5,  5,  7,\n",
      "        11,  9,  5, 15,  8, 15,  5, 12,  7,  9,  5,  5,  5, 12,  5,  9,  9,  5,\n",
      "         5,  5, 10, 12,  9,  5, 11, 13,  9, 15,  5, 12, 16,  9,  5, 14, 13,  6,\n",
      "         5,  7,  9,  9,  5, 10, 10,  7, 10,  9, 14,  9,  9, 11,  5,  9,  5,  9,\n",
      "        11, 15, 13, 10])\n"
     ]
    }
   ],
   "source": [
    "# This is how a batch is retrieved\n",
    "for batch in train_loader:\n",
    "   exps_padded, exp_lengths, funcs_in_padded, funcs_out_padded, func_lengths = batch\n",
    "   \n",
    "   print(\"\\n--- Example Batch ---\")\n",
    "   print(f\"Expansions (padded): \\n{exps_padded}\")\n",
    "   print(f\"Expansions lengths: {exp_lengths}\")\n",
    "   print(f\"Function Inputs (padded): \\n{funcs_in_padded}\")\n",
    "   print(f\"Function Outputs (padded): \\n{funcs_out_padded}\")\n",
    "   print(f\"Function lengths: {func_lengths}\")\n",
    "   break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this task, we can use a basic sequence-to-sequence (seq2seq) model with LSTMs. This consists of:\n",
    "An Encoder: Converts the Taylor expansion into a context vector,\n",
    "A Decoder: Uses the context vector to generate the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout = 0.4\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_seqs, seq_lengths):\n",
    "        # input_seqs: [B, T]\n",
    "        embedded = self.embedding(input_seqs)  # [B, T, embed_dim]\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded,\n",
    "            seq_lengths.cpu(),  \n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        _, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.4\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, input_seqs, hidden, cell):\n",
    "        # input_seqs: [B, T_dec]\n",
    "        embedded = self.embedding(input_seqs)  # [B, T_dec, embed_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        logits = self.fc(output)  # [B, T_dec, vocab_size]\n",
    "        return logits, hidden, cell\n",
    "\n",
    "# Simple wrapper to batch everything\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        \"\"\"\n",
    "        src: [B, T_enc]\n",
    "        src_lengths: [B]\n",
    "        trg: [B, T_dec], where T_dec includes <SOS> at the beginning\n",
    "        \"\"\"\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "        outputs, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS backend.\n",
      "[Epoch 1/10] loss = 0.4032\n",
      "[Epoch 2/10] loss = 0.0739\n",
      "[Epoch 3/10] loss = 0.0412\n",
      "[Epoch 4/10] loss = 0.0329\n",
      "[Epoch 5/10] loss = 0.0317\n",
      "[Epoch 6/10] loss = 0.0313\n",
      "[Epoch 7/10] loss = 0.0281\n",
      "[Epoch 8/10] loss = 0.0288\n",
      "[Epoch 9/10] loss = 0.0331\n",
      "[Epoch 10/10] loss = 0.0275\n"
     ]
    }
   ],
   "source": [
    "# Hyperparam section\n",
    "VOCAB_SIZE = vocab_size\n",
    "EMBED_DIM = 150\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# MPS check(I trained it on my laptop:) )\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon MPS backend.\")\n",
    "\n",
    "\n",
    "# Model definition\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    pad_idx=PAD_TOKEN\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    pad_idx=PAD_TOKEN\n",
    ")\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "#-----\n",
    "# Optimization setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "#Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # break\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        exps_padded, exp_lengths, funcs_in_padded, funcs_out_padded, func_lengths = [\n",
    "            b.to(device) for b in batch\n",
    "        ]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(exps_padded, exp_lengths, funcs_in_padded)\n",
    "        B, T_dec, V = logits.shape\n",
    "        #Compute loss\n",
    "        loss = criterion(logits.view(B*T_dec, V), funcs_out_padded.view(B*T_dec))\n",
    "        \n",
    "        #Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] loss = {avg_loss:.4f}\")\n",
    "    scheduler.step(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"seq2seq_model.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \n",
    "    'model_state_dict': model.state_dict(),\n",
    "    \n",
    "}, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Function:  exp ( x ) * sin ( x )\n",
      "True Function:  exp ( x ) * sin ( x )\n",
      "#####\n",
      "Predicted Function:  x * ( 1 - x ) * exp ( x )\n",
      "True Function:  x * ( 1 - x ) * exp ( x )\n",
      "#####\n",
      "Predicted Function:  - x + exp ( x )\n",
      "True Function:  - x + exp ( x )\n",
      "#####\n",
      "Predicted Function:  - x ^ 3 + sqrt ( 2 ) * sin ( x + pi / 4 )\n",
      "True Function:  - x ^ 3 + sqrt ( 2 ) * sin ( x + pi / 4 )\n",
      "#####\n",
      "Predicted Function:  x ^ 3 * sin ( x )\n",
      "True Function:  x ^ 3 * sin ( x )\n",
      "#####\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, expansion_token_ids, max_length=50):\n",
    "    \"\"\"\n",
    "    Decode a single example (list of token IDs) using the trained model.\n",
    "    Return a list of predicted token IDs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        src_tensor = torch.tensor([expansion_token_ids], dtype=torch.long, device=device)\n",
    "        src_length = torch.tensor([len(expansion_token_ids)], dtype=torch.long, device=device)\n",
    "        \n",
    "        hidden, cell = model.encoder(src_tensor, src_length)\n",
    "        \n",
    "        \n",
    "        dec_input = torch.tensor([[SOS_TOKEN]], dtype=torch.long, device=device)\n",
    "        decoded_ids = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits, hidden, cell = model.decoder(dec_input, hidden, cell)\n",
    "            \n",
    "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1)  # shape: [1]\n",
    "            ntid = next_token_id.item()\n",
    "            if ntid == EOS_TOKEN:\n",
    "                break\n",
    "            decoded_ids.append(ntid)\n",
    "            \n",
    "            \n",
    "            dec_input = next_token_id.unsqueeze(0)  # Now shape is [1,1]\n",
    "            \n",
    "    return decoded_ids\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    Generates a random function, computes its Taylor expansion, tokenizes it, \n",
    "    performs greedy decoding, and prints results\n",
    "    \"\"\"\n",
    "   \n",
    "    f = generate_random_function()\n",
    "    \n",
    "    #print(f\"Target fucntion (Symbolic Representation):  {f}\")\n",
    "\n",
    "    expansion = compute_taylor_expansion(f, x, order=4)\n",
    "    \n",
    "    expansion_tokens = tokenize(expansion)\n",
    "    expansion_ids = [vocab[t] if t in vocab else UNK_TOKEN for t in expansion_tokens]\n",
    "\n",
    "    predicted_token_ids = greedy_decode(model, expansion_ids, max_length=50)\n",
    "\n",
    "    #print(f\"Predicted Token IDs using the model: {predicted_token_ids}\")\n",
    "\n",
    "    predicted_function_tokens = []\n",
    "    for token_id in predicted_token_ids:\n",
    "        if token_id in idx_to_token:\n",
    "            predicted_function_tokens.append(idx_to_token[token_id])\n",
    "        else:\n",
    "            predicted_function_tokens.append(\"[UNK]\")  \n",
    "   \n",
    "    #print(f\"Predicted Tokens:{predicted_function_tokens}\")\n",
    "    \n",
    "    predicted_function_str = \" \".join(predicted_function_tokens)\n",
    "\n",
    "    true_function_str = \" \".join(tokenize(str(f)))\n",
    "\n",
    "    print(f\"Predicted Function:  {predicted_function_str}\")\n",
    "    print(f\"True Function:  {true_function_str}\")\n",
    "    print(\"#####\")\n",
    "\n",
    "\n",
    "for i in range(5):  \n",
    "    evaluate_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After few tries we obtain good result using the LSTM. Sometimes we have allucination, this can be resolved improving the dataset and also by adjusting the distribution of the function in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Tommaso Vaccari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
